{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, 'K:/UZH/Spring 24/Sem Project/kneedeeppose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import submitit\n",
    "import torch\n",
    "from diffdrr.drr import DRR\n",
    "from diffdrr.metrics import MultiscaleNormalizedCrossCorrelation2d\n",
    "from pytorch_transformers.optimization import WarmupCosineSchedule\n",
    "from timm.utils.agc import adaptive_clip_grad as adaptive_clip_grad_\n",
    "from tqdm import tqdm\n",
    "\n",
    "from diffpose.deepfluoro import DeepFluoroDataset, Transforms, get_random_offset\n",
    "from diffpose.metrics import DoubleGeodesic, GeodesicSE3\n",
    "from diffpose.registration import PoseRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(id_number, height, device):\n",
    "    specimen = DeepFluoroDataset(id_number)\n",
    "    isocenter_pose = specimen.isocenter_pose.to(device)\n",
    "\n",
    "    subsample = (1536 - 100) / height\n",
    "    delx = 0.194 * subsample\n",
    "    drr = DRR(\n",
    "        specimen.volume,\n",
    "        specimen.spacing,\n",
    "        specimen.focal_len / 2,\n",
    "        height,\n",
    "        delx,\n",
    "        x0=specimen.x0,\n",
    "        y0=specimen.y0,\n",
    "        reverse_x_axis=True,\n",
    "    ).to(device)\n",
    "    transforms = Transforms(height)\n",
    "\n",
    "    return specimen, isocenter_pose, transforms, drr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    id_number,\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    drr,\n",
    "    transforms,\n",
    "    specimen,\n",
    "    isocenter_pose,\n",
    "    device,\n",
    "    batch_size,\n",
    "    n_epochs,\n",
    "    n_batches_per_epoch,\n",
    "    model_params,\n",
    "):\n",
    "    metric = MultiscaleNormalizedCrossCorrelation2d(eps=1e-4)\n",
    "    geodesic = GeodesicSE3()\n",
    "    double = DoubleGeodesic(drr.detector.sdr)\n",
    "    contrast_distribution = torch.distributions.Uniform(1.0, 10.0)\n",
    "\n",
    "    best_loss = torch.inf\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        losses = []\n",
    "        for _ in (itr := tqdm(range(n_batches_per_epoch), leave=False)):\n",
    "            contrast = contrast_distribution.sample().item()\n",
    "            offset = get_random_offset(batch_size, device)\n",
    "            pose = isocenter_pose.compose(offset)\n",
    "            img = drr(None, None, None, pose=pose, bone_attenuation_multiplier=contrast)\n",
    "            img = transforms(img)\n",
    "\n",
    "            pred_offset = model(img)\n",
    "            # Combine the offset to get actual pose\n",
    "            pred_pose = isocenter_pose.compose(pred_offset)\n",
    "            # Render the img using DRR\n",
    "            pred_img = drr(None, None, None, pose=pred_pose)\n",
    "            pred_img = transforms(pred_img)\n",
    "\n",
    "            ncc = metric(pred_img, img)\n",
    "            log_geodesic = geodesic(pred_pose, pose)\n",
    "            geodesic_rot, geodesic_xyz, double_geodesic = double(pred_pose, pose)\n",
    "            loss = 1 - ncc + 1e-2 * (log_geodesic + double_geodesic)\n",
    "            if loss.isnan().any():\n",
    "                print(\"Aaaaaaand we've crashed...\")\n",
    "                print(ncc)\n",
    "                print(log_geodesic)\n",
    "                print(geodesic_rot)\n",
    "                print(geodesic_xyz)\n",
    "                print(double_geodesic)\n",
    "                print(pose.get_matrix())\n",
    "                print(pred_pose.get_matrix())\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"model_state_dict\": model.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"height\": drr.detector.height,\n",
    "                        \"epoch\": epoch,\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"n_epochs\": n_epochs,\n",
    "                        \"n_batches_per_epoch\": n_batches_per_epoch,\n",
    "                        \"pose\": pose.get_matrix().cpu(),\n",
    "                        \"pred_pose\": pred_pose.get_matrix().cpu(),\n",
    "                        \"img\": img.cpu(),\n",
    "                        \"pred_img\": pred_img.cpu()\n",
    "                        **model_params,\n",
    "                    },\n",
    "                    f\"checkpoints/specimen_{id_number:02d}_crashed.ckpt\",\n",
    "                )\n",
    "                raise RuntimeError(\"NaN loss\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            adaptive_clip_grad_(model.parameters())\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            losses.append(loss.mean().item())\n",
    "\n",
    "            # Update progress bar\n",
    "            itr.set_description(f\"Epoch [{epoch}/{n_epochs}]\")\n",
    "            itr.set_postfix(\n",
    "                geodesic_rot=geodesic_rot.mean().item(),\n",
    "                geodesic_xyz=geodesic_xyz.mean().item(),\n",
    "                geodesic_dou=double_geodesic.mean().item(),\n",
    "                geodesic_se3=log_geodesic.mean().item(),\n",
    "                loss=loss.mean().item(),\n",
    "                ncc=ncc.mean().item(),\n",
    "            )\n",
    "\n",
    "            prev_pose = pose\n",
    "            prev_pred_pose = pred_pose\n",
    "\n",
    "        losses = torch.tensor(losses)\n",
    "        tqdm.write(f\"Epoch {epoch + 1:04d} | Loss {losses.mean().item():.4f}\")\n",
    "        if losses.mean() < best_loss and not losses.isnan().any():\n",
    "            best_loss = losses.mean().item()\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"height\": drr.detector.height,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"loss\": losses.mean().item(),\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"n_epochs\": n_epochs,\n",
    "                    \"n_batches_per_epoch\": n_batches_per_epoch,\n",
    "                    **model_params,\n",
    "                },\n",
    "                f\"checkpoints/specimen_{id_number:02d}_best.ckpt\",\n",
    "            )\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"height\": drr.detector.height,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"loss\": losses.mean().item(),\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"n_epochs\": n_epochs,\n",
    "                    \"n_batches_per_epoch\": n_batches_per_epoch,\n",
    "                    **model_params,\n",
    "                },\n",
    "                f\"checkpoints/specimen_{id_number:02d}_epoch{epoch:03d}.ckpt\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    id_number,\n",
    "    height=256,\n",
    "    restart=None,\n",
    "    model_name=\"resnet18\",\n",
    "    parameterization=\"se3_log_map\",\n",
    "    convention=None,\n",
    "    lr=1e-3,\n",
    "    batch_size=8,\n",
    "    n_epochs=1000,\n",
    "    n_batches_per_epoch=100,\n",
    "):\n",
    "    id_number = int(id_number)\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    specimen, isocenter_pose, transforms, drr = load(id_number, height, device)\n",
    "\n",
    "    model_params = {\n",
    "        \"model_name\": model_name,\n",
    "        \"parameterization\": parameterization,\n",
    "        \"convention\": convention,\n",
    "        \"norm_layer\": \"groupnorm\",\n",
    "    }\n",
    "    model = PoseRegressor(**model_params)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    if restart is not None:\n",
    "        ckpt = torch.load(restart)\n",
    "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "    model = model.to(device)\n",
    "\n",
    "    scheduler = WarmupCosineSchedule(\n",
    "        optimizer,\n",
    "        5 * n_batches_per_epoch,\n",
    "        n_epochs * n_batches_per_epoch - 5 * n_batches_per_epoch,\n",
    "    )\n",
    "\n",
    "    train(\n",
    "        id_number,\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        drr,\n",
    "        transforms,\n",
    "        specimen,\n",
    "        isocenter_pose,\n",
    "        device,\n",
    "        batch_size,\n",
    "        n_epochs,\n",
    "        n_batches_per_epoch,\n",
    "        model_params,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1038115883.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    Path(\"checkpoints\").mkdir(exist_ok=True)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "id_numbers = [1, 2, 3, 4, 5, 6]\n",
    "    Path(\"checkpoints\").mkdir(exist_ok=True)\n",
    "\n",
    "    executor = submitit.AutoExecutor(folder=\"logs\")\n",
    "    executor.update_parameters(\n",
    "        name=\"deepfluoro\",\n",
    "        gpus_per_node=1,\n",
    "        mem_gb=43.5,\n",
    "        slurm_array_parallelism=len(id_numbers),\n",
    "        slurm_partition=\"A6000\",\n",
    "        slurm_exclude=\"sumac,fennel\",\n",
    "        timeout_min=10_000,\n",
    "    )\n",
    "    jobs = executor.map_array(main, id_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "main(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepFluoro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
