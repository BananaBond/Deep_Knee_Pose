# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/api/05_metrics.ipynb.

# %% ../notebooks/api/05_metrics.ipynb 3
from __future__ import annotations

import torch
import torch.nn as nn

# %% auto 0
__all__ = ['NormalizedCrossCorrelation2d', 'NormalizedCrossCorrelation2dMasked', 'MultiscaleNormalizedCrossCorrelation2d', 'GradientNormalizedCrossCorrelation2d']

# %% ../notebooks/api/05_metrics.ipynb 4
class NormalizedCrossCorrelation2d(torch.nn.Module):
    """Compute Normalized Cross Correlation between two batches of images."""

    def __init__(self, patch_size=None, eps=1e-5):
        super().__init__()
        self.patch_size = patch_size
        self.eps = eps

    def forward(self, x1, x2):
        if self.patch_size is not None:
            x1 = to_patches(x1, self.patch_size)
            x2 = to_patches(x2, self.patch_size)
        assert x1.shape == x2.shape, "Input images must be the same size"
        _, c, h, w = x1.shape

        score = torch.einsum("b...,b...->b", x1, x2)
        score /= c * h * w
        return score

    def norm(self, x):
        mu = x.mean(dim=[-1, -2], keepdim=True)
        var = x.var(dim=[-1, -2], keepdim=True, correction=0) + self.eps
        std = var.sqrt()
        return (x - mu) / std


from einops import rearrange

class NormalizedCrossCorrelation2dMasked(torch.nn.Module):
    """Compute Normalized Cross Correlation between two batches of images."""

    def __init__(self, patch_size=None, eps=1e-5):
        super().__init__()
        self.patch_size = patch_size
        self.eps = eps

    def forward(self, x1, x2):
        if self.patch_size is not None:
            x1 = to_patches(x1, self.patch_size)
            x2 = to_patches(x2, self.patch_size)

        assert x1.shape == x2.shape, "Input images must be the same size"
        
        # Normalize the inputs
    
        # Create mask for non-zero pixels in both images
        mask = (x1 > 0) & (x2 > 0)
        # print('mask sum', mask.sum())
        total_mask = (x1 > 0) | (x2 > 0)

        x1_masked, x2_masked = self.normalize_masked(x1, mask), self.normalize_masked(x2, mask)
        
        # # Apply the mask to both images
        # x1_masked = x1 * mask
        # x2_masked = x2 * mask
        
        # Calculate the number of valid pixels (non-zero pixels)
        valid_pixel_count = mask.sum(dim=[1, 2, 3])  # sum over the spatial dimensions (C, H, W)
        total_valid_pixel_count = total_mask.sum(dim=[1, 2, 3])
        
        # Ensure no division by zero in case there are no valid pixels
        valid_pixel_count = torch.clamp(valid_pixel_count, min=1)
        # print("valid_pixel_count", valid_pixel_count)
        total_valid_pixel_count = torch.clamp(total_valid_pixel_count, min=1)
        # print("total_valid_pixel_count", total_valid_pixel_count)


        
        # Compute the MNCC score only for non-zero pixels
        score = torch.einsum("b...,b...->b", x1_masked, x2_masked)
        # print(score)
        
        # Normalize the score by the number of valid pixels
        # score /= valid_pixel_count.float()

        # --- Additional NCC computation for x2 with itself ---
    
        # Compute the MNCC score for x2 with itself (self-similarity)
        # x2_mask = x2 > 0
        x2_self = self.normalize_masked(x2, mask)

        # x1_mask = x2 > 0
        x1_self = self.normalize_masked(x1, mask)

        score_x2_self = torch.einsum("b...,b...->b", x2_self, x2_self)
        score_x1_self = torch.einsum("b...,b...->b", x1_self, x1_self)

        # valid_pixel_count_x2_self = x2_mask.sum(dim=[1, 2, 3])

        # score_x2_self /= valid_pixel_count_x2_self.float()
        
        # Normalize the self-score by the number of valid pixels (since we're masking)
        # score_x2_self /= score_x2_self

        denominator = torch.sqrt(score_x1_self * score_x2_self) + self.eps
        # print(denominator)
        # return score / denominator 
        return score / denominator * valid_pixel_count / total_valid_pixel_count
    
        # return score / score_x2_self, x1_masked, x2_masked

    def norm(self, x):
        mu = x.mean(dim=[-1, -2], keepdim=True)
        var = x.var(dim=[-1, -2], keepdim=True, correction=0) + self.eps
        std = var.sqrt()
        return (x - mu) / std

    def normalize_masked(self, x, mask):
        # Only consider the non-zero (masked) pixels for mean and std calculation
        masked_x = x[mask]
        if masked_x.numel() > 1:  # If there are masked pixels
            mean = masked_x.mean()
            std = masked_x.std()
            if std < self.eps:
                std = self.eps  # Avoid division by very small std
        else:  # If the mask is empty (no non-zero pixels), avoid division by zero
            mean = torch.zeros_like(x)
            std = torch.ones_like(x)

        x_normalized = (x - mean) / (std + self.eps)  # Small epsilon to prevent division by zero

        # Return the normalized values, with non-masked pixels unchanged
        return (x_normalized * mask.float() + self.eps) + x




# class NormalizedCrossCorrelation2dMasked(torch.nn.Module):
#     """Compute Normalized Cross Correlation between two batches of images."""

#     def __init__(self, patch_size=None, eps=1e-5, pixel_thresh = 1000):
#         super().__init__()
#         self.patch_size = patch_size
#         self.eps = eps
#         self.pixel_threshold = pixel_thresh

#     # def forward(self, x1, x2):
#     #     if self.patch_size is not None:
#     #         x1 = to_patches(x1, self.patch_size)
#     #         x2 = to_patches(x2, self.patch_size)
#     #     assert x1.shape == x2.shape, "Input images must be the same size"
#     #     _, c, h, w = x1.shape
        
#     #     mask = (x1 - x2).abs() > self.eps  # Ignore pixels where the difference is close to 0 (perfect match)
#     #     x1_pixels = torch.sum(x1>0).abs()
#     #     x2_pixels = torch.sum(x2>0).abs()

#     #     if x1_pixels < self.pixel_threshold or x2_pixels < self.pixel_threshold:
#     #         print("x1_pixels = ", x1_pixels, x2_pixels)
#     #         return 0


#     #     x1, x2 = self.norm(x1), self.norm(x2)
#     #     # print("mask sum = ", torch.sum(mask))
#     #     # Apply the mask to both x1 and x2

#     #     score = torch.einsum("b...,b...->b", x1, x2)
#     #     score /= c * h * w 
#     #     valid_pixels = mask.sum(dim=(1, 2, 3), keepdim=False).clamp(min=1)
#     #     if valid_pixels < min(x1_pixels, x2_pixels):
#     #         valid_pixels = min(x1_pixels, x2_pixels)
#     #     score /= valid_pixels
#     #     score *= self.patch_size*self.patch_size
#     #     return score
    

#     def forward(self, x1, x2):
#         # If patch_size is provided, convert images into patches
#         if self.patch_size is not None:
#             x1 = to_patches(x1, self.patch_size)
#             x2 = to_patches(x2, self.patch_size)

#         # Ensure x1 and x2 have the same shape
#         assert x1.shape == x2.shape, "Input images must be the same size"
        
#         # Get batch size, channels, height, and width
#         batch_size, c, h, w = x1.shape
        
#         # Create mask to ignore pixels where the difference is close to 0 (perfect match)
#         mask = (x1 - x2).abs() > self.eps  # Shape: (batch_size, c, h, w)
        
#         # Calculate number of non-zero pixels for each image in the batch
#         x1_pixels = torch.sum(x1 > 0, dim=(1, 2, 3)).abs()  # Shape: (batch_size,)
#         x2_pixels = torch.sum(x2 > 0, dim=(1, 2, 3)).abs()  # Shape: (batch_size,)
#         # print("x1_pixels", x1_pixels)
#         # print("x1_pixels", x2_pixels)
#         max_valid_pixels = x1_pixels + x2_pixels
#         # print("max_valid_pixels", max_valid_pixels)
#         # print("max_valid_pixels.shape", max_valid_pixels.shape)


#         # Batch-wise threshold check for valid pixel count
#         valid_mask = (x1_pixels >= self.pixel_threshold) | (x2_pixels >= self.pixel_threshold)
#         # print('valid_mask', valid_mask)
        
#         # Initialize scores for all images in the batch
#         score = torch.zeros(batch_size, dtype=x1.dtype, device=x1.device)

#         # Only process the valid images in the batch
#         if valid_mask.any():  # If there are any valid images
#             # Normalize only the valid images
#             x1_valid = self.norm(x1[valid_mask])
#             x2_valid = self.norm(x2[valid_mask])

#             # Compute NCC score for each valid image
#             score_valid = torch.einsum("b...,b...->b", x1_valid, x2_valid)  # Shape: (num_valid,)
#             # score_valid = torch.einsum("b...,b...->b", x1, x2)  # Shape: (num_valid,)

            
#             # Normalize score by the total number of pixels
#             score_valid /= (c * h * w)
            
#             # Count the number of valid pixels in the mask for each valid image
#             valid_pixels = mask[valid_mask].sum(dim=(1, 2, 3), keepdim=False).clamp(min=1)  # Shape: (num_valid,)
#             # valid_pixels = mask.sum(dim=(1, 2, 3), keepdim=False).clamp(min=1)  # Shape: (num_valid,)


#             # print("valid_pixels.shape")
#             # print(valid_pixels.shape)
            
#             # Adjust valid_pixels to ensure it's no greater than the smaller of x1_pixels or x2_pixels
#             # valid_pixels = torch.min(valid_pixels, torch.min(x1_pixels[valid_mask], x2_pixels[valid_mask]))
            
#             # Normalize score by valid pixels
#             # score_valid /= valid_pixels
#             score_valid /= valid_pixels * max_valid_pixels[valid_mask]
#             score_valid = torch.clamp(score_valid, min=self.eps)
        
        
#         # Scale score by patch size if applicable
#         # if self.patch_size:
#         #     score_valid /= (self.patch_size ** 2)
#             # score_valid *= (self.patch_size ** 2)
        
#             # Update the score for valid images in the batch
#             score[valid_mask] = score_valid

#         return score

    # def norm(self, x):
    #     mu = x.mean(dim=[-1, -2], keepdim=True)
    #     var = x.var(dim=[-1, -2], keepdim=True, correction=0) + self.eps
    #     std = var.sqrt()
    #     return (x - mu) / std


class MultiscaleNormalizedCrossCorrelation2d(torch.nn.Module):
    """Compute Normalized Cross Correlation between two batches of images at multiple scales."""

    def __init__(self, patch_sizes=[None], patch_weights=[1.0], eps=1e-5):
        super().__init__()

        assert len(patch_sizes) == len(patch_weights), "Each scale must have a weight"
        self.nccs = [
            NormalizedCrossCorrelation2d(patch_size) for patch_size in patch_sizes
        ]
        self.patch_weights = patch_weights

    def forward(self, x1, x2):
        scores = []
        for weight, ncc in zip(self.patch_weights, self.nccs):
            scores.append(weight * ncc(x1, x2))
        return torch.stack(scores, dim=0).sum(dim=0)

class MultiscaleNormalizedCrossCorrelation2dMasked(torch.nn.Module):
    """Compute Normalized Cross Correlation between two batches of images at multiple scales."""

    def __init__(self, patch_sizes=[None], patch_weights=[1.0], eps=1e-5):
        super().__init__()

        assert len(patch_sizes) == len(patch_weights), "Each scale must have a weight"
        self.nccs = [
            NormalizedCrossCorrelation2dMasked(patch_size) for patch_size in patch_sizes
        ]
        self.patch_weights = patch_weights

    def forward(self, x1, x2):
        scores = []
        for weight, ncc in zip(self.patch_weights, self.nccs):
            scores.append(weight * ncc(x1, x2))
        return torch.stack(scores, dim=0).sum(dim=0)
    
# %% ../notebooks/api/05_metrics.ipynb 5
from einops import rearrange


def to_patches(x, patch_size):
    x = x.unfold(2, patch_size, step=12).unfold(3, patch_size, step=12).contiguous()
    return rearrange(x, "b c p1 p2 h w -> b (c p1 p2) h w")

# %% ../notebooks/api/05_metrics.ipynb 6
class GradientNormalizedCrossCorrelation2d(NormalizedCrossCorrelation2d):
    """Compute Normalized Cross Correlation between the image gradients of two batches of images."""

    def __init__(self, patch_size=None, sigma=1.0, **kwargs):
        super().__init__(patch_size, **kwargs)
        self.sobel = Sobel(sigma)

    def forward(self, x1, x2):
        return super().forward(self.sobel(x1), self.sobel(x2))

# %% ../notebooks/api/05_metrics.ipynb 7
from torchvision.transforms.functional import gaussian_blur


class Sobel(torch.nn.Module):
    def __init__(self, sigma):
        super().__init__()
        self.sigma = sigma
        self.filter = torch.nn.Conv2d(
            in_channels=1,
            out_channels=2,  # X- and Y-gradients
            kernel_size=3,
            stride=1,
            padding=1,  # Return images of the same size as inputs
            bias=False,
        )

        Gx = torch.tensor([[1, 0, -1], [2, 0, -2], [1, 0, -1]]).to(torch.float32)
        Gy = torch.tensor([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]).to(torch.float32)
        G = torch.stack([Gx, Gy]).unsqueeze(1)
        self.filter.weight = torch.nn.Parameter(G, requires_grad=False)

    def forward(self, img):
        x = gaussian_blur(img, 5, self.sigma)
        x = self.filter(img)
        return x
